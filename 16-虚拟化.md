

# 系统虚拟化

**系统虚拟化的优势** （P330）

1. 服务器整合
2. 虚拟机管理
3. 虚拟机热迁移
4. 虚拟机安全自省
5. 方便程序开发

**虚拟化标准**

1. 为虚拟机内程序提供与该程序原先执行的硬件完全一样的接口 
2. 虚拟机只比在无虚拟化的情况下性能略差一点 
3. 虚拟机监控器控制所有物理资源



## 1. 虚拟机分类

### 1.1 Type-1虚拟机监控器

* 直接运行在硬件之上 
  * 充当操作系统的角色 
  * 直接管理所有物理资源 
    * 实现调度、内存管理、驱动等功能 
* 性能损失较少 
* 例如Xen, VMware ESX Server

![type1](图片\type1.png)![type2](图片\type2.png)

### 1.2 Type-2虚拟机监控器

* 依托于主机操作系统 
  * 主机操作系统管理物理资源 
  * 虚拟机监控器以进程/内核模块的形态运行 
* 易于实现和安装 
* 例如QEMU/KVM

**优势**

1. 在已有的操作系统之上将虚拟机当做应用运行

2. 复用主机操作系统的大部分功能

   文件系统 – 驱动程序 – 处理器调度 – 物理内存管理



## 2. 实现虚拟化

**系统虚拟化的流程**

1. 第一步 ：捕捉所有系统ISA并陷入(Trap)

2. 第二步 ：由具体指令实现相应虚拟化

   • 控制虚拟处理器行为 • 控制虚拟内存行为 • 控制虚拟设备行为

3. 第三步 ： 回到虚拟机继续执行



## 3. 处理器虚拟化

### 下陷和模拟

🔺 将虚拟机监控器运行在EL1， 将客户操作系统和其上的进程都运行在EL0 ，当操作系统执行系统ISA指令时下陷

▲ **trap** : 在用户态EL0执行特权指令将陷入EL1的VMM中

▲ **emulate **: 这些指令的功能都由VMM内的函数实现

![下陷和模拟](图片\下陷和模拟.png)

🔺 VMM看到特权指令后修改PC，直接修改寄存器等等，模拟硬件的执行效果->是纯软件模拟哦



**敏感指令**

敏感指令指，<u>管理系统物理资源</u>或<u>更改CPU状态</u>的指令

有以下分类：

1. 读写特殊寄存器或更改处理器状态 

2. 读写敏感内存：例如访问未映射内存、写入只读内存

3. I/O指令

**特权指令**

* 在<u>用户态</u>执行会**触发异常**，并陷入内核态

🔺 **可虚拟化架构**特征： 所有敏感指令都是特权指令，即都会下陷被VMM捕捉



在下陷和模拟的虚拟化办法中，有五个方法可以弥补不可虚拟化的方法：

* 解释执行
* 动态二进制翻译
* 扫描和翻译
* 半虚拟化技术
* 硬件虚拟化技术

### 3.1 解释执行

* 使用软件方法对<u>每一条虚拟机代码</u>进行模拟 
  * 不区分敏感指令还是其他指令 
  * 没有虚拟机指令直接在硬件上执行 
* 使用内存维护虚拟机状态 
  * 例如：使用uint64_t x[30]数组保存所有通用寄存器的值

🔺 所有虚拟机的指令都是VMM软件模拟的，并不是硬件直接执行的



**优点**

1. 解决了敏感函数不下陷的问题 （因为根本不区分敏感指令和普通指令了）
2. 可以模拟不同ISA的虚拟机 
3. 易于实现、复杂度低

**缺点**

1. 非常慢：任何一条虚拟机指令都会转换成多条模拟指令



### 3.2 二进制翻译

扫一遍二进制，如果扫描到敏感指令，则替换，不是敏感指令则不动。二进制就是硬件真的执行的二进制。不用VMM模拟执行。

![二进制翻译](图片\二进制翻译.png)

* 提出两个加速技术 
  * 执行前**批量翻译**虚拟机指令 
  * **缓存**已翻译完成的指令 
* 使用基本块(Basic Block)的翻译粒度（为什么?） 
  * 每一个基本块被翻译完后叫代码补丁

**优点**

因为1. 批量翻译 2. 只针对翻译敏感指令 3. 最终是硬件直接执行 4. 缓存命中

加快了速度

**缺点**

1. 不能处理自修改的代码(Self-modifying Code)

2. 中断插入粒度变大

   – 模拟执行可以在任意指令位置插入虚拟中断 

   – 二进制翻译时只能在<u>基本块边界</u>插入虚拟中断

   ​	（因为只能在每个基本块结束的时候检查有没有中断来，如果是正常系统，是引脚控制，粒度是每个指令）



### 3.3 半虚拟化

* 让VMM提供接口给虚拟机，称为Hypercall
* 修改操作系统源码，替换不可虚拟化的指令，将其更改为Hypercall

▲ 就是将所有不引起下陷的敏感指令替换成超级调用

**优点**

1. 解决了敏感函数不下陷的问题 
2. 协同设计的思想可以提升某些场景下的系统性能 
   * I/O等场景（反正改了操作系统，系统调用可以进行batch优化）

**缺点**

1. 需要修改操作系统代码，难以用于闭源系统 
2. 即使是开源系统，也难以同时在不同版本中实现



### 3.4 硬件虚拟化

#### 3.4.1 VT-x的处理器虚拟化

![VT-x的处理器虚拟化](图片\VT-x的处理器虚拟化.png)

**VMM的进程对应虚拟机**

**VMM的线程对应vcpu**

* 因为线程的引入是为了并行，是调度的基本单位

* 如果给虚拟机配了n个vcpu，那么无论虚拟机里有多少个线程，最终在VMM里真正运行的只有vcpu个

* **spin-lock**:对于虚拟机来说，他以为自己占用了所有的CPU，所以这个线程没锁，会认为锁被别的CPU上的线程拿着，那个人的关键路径肯定很短，会马上放掉，然后我就可以用了

  但事实上，自己只占用vcpu个cpu，所以可能会被调度走，spin很久

##### **3.4.1.1 VMCS**

* VMM提供给硬件的内存页（4KB）
  * 记录与当前VM运行相关的所有状态
* **VM Entry** 
  * 硬件自动将当前CPU中的VMM状态保存至VMCS 
  * 硬件硬件自动从VMCS中加载VM状态至CPU中 
* **VM Exit** 
  * 硬件自动将当前CPU中的VM状态保存至VMCS 
  * 硬件自动从VMCS加载VMM状态至CPU中

**包含6个部分** 

1. Guest-state area: 发生VM exit时，CPU的状态会被硬件自动保存至 该区域；发生VM Entry时，硬件自动从该区域加载状态至CPU中 
2. Host-state area：发生VM exit时，硬件自动从该区域加载状态至 CPU中；发生VM Entry时，CPU的状态会被自动保存至该区域 
3. VM-execution control fields： 控制Non-root模式中虚拟机的行为 
4. VM-exit control fields：控制VM exit的行为 （▲ 可配置系统调用是guest os直接处理还是vm exit给VMM处理）
5. VM-entry control fields：控制VM entry的行为 
6. VM-exit information fields：VM Exit的原因和相关信息（只读区域）

![vt-x执行过程](图片\vt-x执行过程.png)

* 有几个虚拟机就有几个VMCS
* 但是真正加载到CPU里面的VMCS的数量不超过系统的总核数



##### 3.4.2 VM Entry

* 从VMM进入VM 
* 从Root模式切换到Non-root模式 
* 第一次启动虚拟机时使用VMLAUNCH指令 
* 后续的VM Entry使用VMRESUME指令



##### 3.4.3 VM Exit

* 从VM回到VMM 
* 从Non-root模式切换到Root模式 
* 虚拟机执行敏感指令或发生事件(如外部中断)

![vt-x和普通进程对比](图片\vt-x和普通进程对比.png)

#### 3.4.2 ARM的虚拟化技术1

<img src="图片\arm虚拟化1.png" alt="arm虚拟化1" style="zoom: 40%;" /><img src="图片\arm虚拟化2.png" alt="arm虚拟化2" style="zoom: 40%;" />

**VM Entry**

* 使用ERET指令从VMM进入VM 
* 在进入VM之前，VMM需要主动加载VM状态 
  *  VM内状态：通用寄存器、系统寄存器
  *  VM的控制状态：HCR_EL2、VTTBR_EL2等

**VM Exit**

* 虚拟机执行敏感指令或收到中断等
* 以Exception、IRQ、FIQ的形式回到VMM
  * 调用VMM记录在vbar_el2中的相关处理函数
* 下陷第一步：VMM主动保存所有VM的状态



**ARM硬件虚拟化的新功能**

* ARM中没有VMCS
* VM能直接控制EL1和EL0的状态
  * 自由地修改PSTATE(VMM不需要捕捉CPS指令)
  * 可以读写TTBR0_EL1/SCTRL_EL1/TCR_EL1等寄存器
* VM Exit时VMM仍然可以直接访问VM的EL0和EL1寄存器

▲ 因为ARM的寄存器有两套，所以切换不用放到VMCS里保存

**问题**

* 增加EL21特权级
* EL2只能运行VMM，不能运行一般操作系统内核
  * OS一般只使用EL1的寄存器，在EL2中不存在对应的寄存器
    * EL1：TTBR0_EL1、TTBR1_EL1
    * EL2：TTBR_EL2
  * EL2不能与EL0共享内存
* 因此：无法在EL2中运行Type-2虚拟机监控器的Host OS
  * 或者说，Host OS需要大量修改才能运行在EL2



#### 3.4.3 ARM的虚拟化技术2

**ARMv8.1** 

* 推出Virtualization Host Extensions(**VHE**)，在HCR_EL2.E2H打开 
  * 寄存器映射 
  * 允许与EL0共享内存 
* 使EL2中可直接运行未修改的操作系统内核（Host OS）

![ARMv8.1](图片\ARMv8.1.png)

![ARMv8.1中的Type-2 VMM架构](图片\ARMv8.1中的Type-2 VMM架构.png)

#### 3.4.4 VT-x和VHE对比

![VT-x和VHE对比](图片\VT-x和VHE对比.png)

#### 3.4.5 Type-1和Type-2在VT-x和VHE下架构

![Type-1和Type-2在VT-x和VHE下架构](图片\Type-1和Type-2在VT-x和VHE下架构.png)



## 4. QEMU/KVM

**QUME**

* 运行在<u>用户态</u>，负责实现<u>策略</u>
* 也提供虚拟设备的支持

**KVM**

* 以<u>Linux内核</u>模块运行，负责实现<u>机制</u>
* 可以直接使用Linux的功能 
* 例如内存管理、进程调度 
* 使用硬件虚拟化功能

**两部分合作**

* KVM捕捉所有敏感指令和事件（trap），传递给QEMU(emulate)
* KVM不提供设备的虚拟化，需要使用QEMU的虚拟设备

### 4.1 QEMU使用KVM的用户态接口

![QEMU使用KVM的用户态接口](图片\QEMU使用KVM的用户态接口.png)

**ioctl(KVM_RUN)时发生了什么**

* x86中 

  1. KVM找到此VCPU对应的VMCS
  2. 使用指令加载VMCS
  3.  VMLAUNCH/VMRESUME进入Non-root模式 
  4. 硬件自动同步状态 (更新VMCS)
  5. PC切换成VMCS->GUEST_RIP，开始执行

* ARM中

  1. KVM主动加载VCPU对应的所有状态

  2. 使用eret指令进入EL2

     PC切换成ELR_EL2的值，开始执行



### 4.2 QEMU/KVM的流程

![QEMU-KVM的流程](图片\QEMU-KVM的流程.png)

▲ KVM 有两个switch case，一个是等待QEMU调ioctl(对参数switch)，另一个等待虚拟机的VM exit

```c
open("/dev/kvm")
ioctl(KVM_CREATE_VM)
ioctl(KVM_CREATE_VCPU)
while (true) {
	ioctl(KVM_RUN)					// 执行这句后就进入了KVM后进入虚拟机
        							// 一旦这句话返回，说明发生了VM exit
	exit_reason = get_exit_reason();
	switch (exit_reason) {
		case KVM_EXIT_IO: /* ... */
			break;
		case KVM_EXIT_MMIO: /* ... */
			break;
	}
}
```

**WFI指令**VM Exit的处理流程

![WFI指令VM Exit的处理流程](图片\WFI指令VM Exit的处理流程.png)

**I/O指令**VM Exit的处理流程

<img src="图片\IOVM Exit的处理流程.png" alt="IOVM Exit的处理流程" style="zoom:80%;" />



## 5. 内存虚拟化

**内存虚拟化的目标**

1. 为虚拟机提供虚拟的物理地址空间

   物理地址从0开始连续增长

2. 隔离不同虚拟机的物理地址空间

   VM-1无法访问其他的内存

**内存虚拟化特点** ：P342

### 5.1 影子页表

影子页表是在没有硬件虚拟化的条件下

**影子页表的创建**：（具体见P344）

```c
// 构造影子页表
set_cr3 (guest_page_table):
	for GVA in 0 to 220
		if guest_page_table[GVA] & PTE_P:
		// P位被设上说明是已经映射的地址
			GPA = guest_page_table[GVA] >> 12	// 后12位是属性
			HPA = host_page_table[GPA] >> 12
            
            if GVA 被用来做页表，则标记位只读
			shadow_page_table[GVA] = (HPA<<12)|PTE_P
		else
			shadow_page_table[GVA] = 0
	CR3 = PHYSICAL_ADDR(shadow_page_table)
```

**▲ shadow_page_table**保存在host的内存（安全性）

▲ guest只能操作GPT，看不到影子页表

![影子页表](图片\影子页表.png)

🔺 把<u>GPT页表内存页的地址</u>（就是那个虚拟地址）在<u>影子页表</u>里标记为**只读**，这样客户操作系统想改GPT的时候，会触发page fault，并下陷到VMM。VMM帮助更新GPT和SPT

🔺 Guest Os 看自己的页表，也就是GPT，是可读可写的

🔺 虚拟化的关键：**TRAP**



现在虚拟机跑在ring3(低权限)，host跑在ring0。SPT应给给GPT的内存必须是ring3，这样就<u>打破了虚拟机里面的用户态和内核的隔离</u>，（现在虚拟机里的用户态和内核态的内存都是ring3，那用户就可以随意访问内核的内存了）

![guestVM的内存](图片\guestVM的内存.png)

把一个页表拆成两个页表，都是ring3。给应用的页表，不映射内核的数据。

如何在两个页表中切换？

guest 的APP 调syscall 进入host OS  (ring3调syscall进入ring0)，host OS 拿到syscall，切到guest OS。

▲ host OS本质上是trap了guest OS 和 guest APP 的切换，给他们换页表



**优点**

1. 地址翻译速度快

   MMU只需遍历一个页表就可完成翻译，即使TLB未命中，最多只需要四次访存操作

**缺点**

1. 影子页表的<u>建立</u>和后续的每次<u>更新</u>都需要VMM介入
2. 影子页表与页表一一对应，一个进程需要一个影子页表

### 5.2 直接页表映射

▲ 修改guest OS，guest OS的页表直接记录<u>GVA到HPA</u>的映射

▲ VMM需要告知VM允许使用的HPA的范围

▲ 为了安全性（VM不能乱写HPA，写到别人那边去），所以修改页表需要hypercall。将虚拟机的页表页设置为**只读**，虚拟机修改页表需要hypercall，VMM在接收请求后，修改页表，检查是否合法。

**优点**

1. 实现复杂度降低
2. 影子页表将虚拟机对客户页表的修改透明的同步到影子页表，有较大性能开销，直接页表映射虚拟机可以将对页表的多次修改batch

**缺点**

1. 要修改虚拟机OS
2. guest知道的太多了（安全问题roll hammer）

### 5.3 两阶段地址翻译

硬件虚拟化❗

![第二阶段4级页表](图片\第二阶段4级页表.png)

![两阶段地址翻译](图片\两阶段地址翻译.png)



与影子页表不同，影子页表虽然也会维护GPA到HPA的映射，但是这个转换表在MMU翻译时除了维护信息没有其他作用。而这里EL2的页表可以直接被MMU识别并参与翻译

在虚拟机执行过程中，任何的GPA都会被硬件MMU通过`VTTBR_EL2`指向的第二级页表翻译成对应的HPA，整个过程无需VMM介入

**地址翻译过程**

1. 先把`TTBR0_EL1`（GPA）（也就是第一级页表的L0页表地址），翻译成HPA_L0(这个HPA就是第一级页表的L0页表页的宿主机上的物理地址)
2. 然后拿着GVA的在L0页的offset，通过HPA_L0 + offset 得到L1页表的GPA
3. 用L1页表的GPA通过第二级页表翻译成HPA_L1
4. 然后拿着GVA的在L1页的offset，通过HPA_L1 + offset 得到L2页表的GPA
5. 用L2页表的GPA通过第二级页表翻译成HPA_L2
6. 然后拿着GVA的在L2页的offset，通过HPA_L2+ offset 得到L3页表的GPA
7. 用L3页表的GPA通过第二级页表翻译成HPA_L3
8. 然后拿着GVA的在L3页的offset，通过HPA_L3 + offset 得到GVA对应的HPA

**VMID** (Virtual Machine IDentifier) 

* VMM为不同进程分配8/16 VMID，将VMID填写在VTTBR_EL2的高8/16位
* VMID位数由VTCR_EL2的第19位（VS位）决定
* 避免刷新上个VM的TLB

**优点**

1. 客户操作系统在更新页表时不会引起任何虚拟机下陷，性能好
2. 减少内存开销，不需要为每一个进程单独配置一个表，一个虚拟机一个第二阶段页表
3. 第一阶段页表和第二阶段页表引起的异常将分别唤醒虚拟机的虚拟机监控器，大大提升了缺页异常的处理性能
4. 不需要捕捉Guest Page Table的更新
5. 可以为一个虚拟机，配不同的第二阶段页表->可以防硬件漏洞

**缺点**

1. 在发生TLB未命中的时候，最差情况下，需要经过24次访存

### 5.4 memory ballooning

🔺 提升性能的关键：打破层次

为了解决虚拟机的内存超售，可以使用内存气球

1. VMM需要回收虚拟机1的内存
2. VMM通知虚拟机1里的气球驱动
3. 气球驱动调用客户操作系统提供的内存分配接口（kmalloc)，分配大量内存
4. 气球驱动在得到这些内存后，将内存对应的GPA发送给VMM
5. VMM翻译为HPA，并把对应的内存数据保存到存储设备



## 6. I/O虚拟化

> 为什么要I/O虚拟化

如果VM直接管理物理网卡：

1. **正确性**问题：所有VM都直接访问网卡

   – 所有VM都有相同的MAC地址、IP地址，无法正常收发网络包

2. **安全性**问题：恶意VM可以直接读取其他VM的数据

   除了直接读取所有网络包，还可能通过DMA访问其他内存



**IO虚拟化目标**

1. 为虚拟机提供虚拟的外部设备
2. 隔离不同虚拟机对外部设备的直接访问
3. 提高物理设备的利用资源



**实现IO虚拟化**

1. 设备模拟（Emulation）（纯软件）
2. 半虚拟化方式（Para-virtualization）
3. 设备直通（Pass-through）



### 6.1 软件模拟

关键：捕捉原生驱动的硬件指令 -> trap and emulate

**发包**

1. VM进行MMIO操作，被trap，导致VM exit
2. 被KVM捕捉后，KVM发现自己处理不了，进QEMU
3. QEMU从VM的memory里读出数据包，调用syscall
4. 进去EL2 ，发给真正的网卡

![设备模拟-发包](图片\设备模拟-发包.png)

**收包**

![设备模拟-收包](图片\设备模拟-收包.png)

**优点**

1. 可以模拟任意设备

   选择流行设备，支持较“久远”的OS（如e1000网卡） 

2. 允许在中间拦截（Interposition）

   例如在QEMU层面检查网络内容

3. 不需要硬件修改

**缺点**

性能不佳



### 6.2 半虚拟化方式

**协同设计**

* 虚拟机“知道”自己运行在虚拟化环境 
* 虚拟机内运行前端(front-end)驱动 
*  VMM内运行后端(back-end)驱动
* 通过共享内存传递指令和命令

**发包**

![半虚拟化-发包](图片\半虚拟化-发包.png)

**收包**

![半虚拟化-收包](图片\半虚拟化-收包.png)

**优点**

1. 性能优越

   多个MMIO/PIO指令可以整合成一次Hypercall，减少VM exit

2. VMM实现简单，不再需要理解物理设备接口

3. QEMU不用实现模拟网卡了

**缺点**

需要修改虚拟机操作系统内核



### 6.3 设备直通

虚拟机直接管理物理设备

#### **问题1**

DMA恶意读写内存

![设备直通-DMA恶意读写内存](图片\设备直通-DMA恶意读写内存.png)

做DMA的地址是HPA，会绕过页表，就不能做权限控制了。

**解决问题1**

加页表（IO的页表）

![设备直通-DMA恶意读写内存-解决](图片\设备直通-DMA恶意读写内存-解决.png)

IO VA就是GPA（IO的位置是guest OS 设的所以是GPA）

**IOMMU**把GPA翻译成HPA（因为真的设备在HPA）



#### 问题2

设备独占

* Scalability不够 
  * 设备被VM-1独占后，就无法被VM-2使用 
* 如果一台物理机上运行16个虚拟机
  *  必须为这些虚拟机安装16个物理网卡

**解决-问题2**

▲ SR-IOV : 在设备层实现虚拟化（实现在设备上）

* SR-IOV是PCI-SIG组织确定的标准
* 满足SRIOV标准的设备，在设备层实现设备复用
  * 能够创建多个Virtual Function(VF)，每一个VF分配给一个VM 
    * 负责进行数据传输，属于**数据面**（Data-plane） 
  * 物理设备被称为Physical Function(PF)，由Host管理
    * 负责进行配置和管理，属于**控制面**（Control-plane）、
  * 一个PF控制多个VF
*  设备的功能
  * 确保VF之间的数据流和控制流彼此不影响

![设备直通-SR-IOV的使用](图片\设备直通-SR-IOV的使用.png)

**优点**

1. 性能优越
2. 简化VMM的设计与实现

**缺点**

1. 需要特定硬件功能的支持（IOMMU、SRIOV等） 
2.  不能实现Interposition：难以支持虚拟机热迁移



### 6.4 I/O虚拟化技术对比

![IO虚拟化技术对比](图片\IO虚拟化技术对比.png)